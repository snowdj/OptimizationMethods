# 机器学习优化函数
* linear_regression.py：线性回归
* grandent_descent.py：梯度下降
* SGD.py：随机梯度下降
* minibatch_SGD.py：minibatch SGD
* momentum.py：动量
* momentum_SGD.py：动量 SGD
* Nesterov_momentum.py
* adagrad.py: adagrad
* adadelta.py: adadelta
* adam.py: adam

#### 来源

代码来源于以下博客系列文章，稍作改动和重构

http://blog.csdn.net/column/details/19920.html

#### 参考

p1 参考 https://zhuanlan.zhihu.com/p/27297638

p2~pn 参考 http://ruder.io/optimizing-gradient-descent/index.html

###### 单独参考

p5 参考 http://cs231n.github.io/neural-networks-3/

p6 参考 https://zhuanlan.zhihu.com/p/22252270

p7 参考 https://arxiv.org/abs/1212.5701 (原始论文)

p8 参考 http://www.ijiandao.com/2b/baijia/63540.html